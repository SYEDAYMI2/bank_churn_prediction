{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":358170,"sourceType":"datasetVersion","datasetId":156197}],"dockerImageVersionId":31239,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":20.502482,"end_time":"2025-09-25T17:28:12.356612","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-25T17:27:51.854130","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9a89f46b","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.058921,"end_time":"2025-09-25T17:28:00.314451","exception":false,"start_time":"2025-09-25T17:27:58.255530","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"c5004dc0","cell_type":"code","source":"# Bank Customer Churn Prediction ML Model\n# Complete implementation with 8 algorithms and ensemble approach\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.pipeline import Pipeline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load and explore data\nprint(\"Loading Bank Churn Dataset...\")\ndata = pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv')\n\n\nprint(f\"Dataset shape: {data.shape}\")\nprint(f\"Target distribution:\\n{data['Exited'].value_counts()}\")\nprint(f\"Churn rate: {data['Exited'].mean()*100:.2f}%\")\n\n# Data preprocessing and feature engineering\ndef preprocess_data(df):\n    \"\"\"\n    Comprehensive data preprocessing and feature engineering\n    \"\"\"\n    # Create copy\n    processed_df = df.copy()\n    \n    # Drop irrelevant columns\n    processed_df = processed_df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n    \n    # Feature Engineering - Create 13 engineered features\n    \n    # 1. Age groups\n    processed_df['AgeGroup'] = pd.cut(\n    processed_df['Age'],\n    bins=[0, 30, 40, 50, 60, 100],\n    labels=[0, 1, 2, 3, 4],\n    include_lowest=True\n).astype(int)\n\n    \n    # 2. Balance categories\n    processed_df['BalanceCategory'] = pd.cut(\n    processed_df['Balance'],\n    bins=[-1, 0, 50000, 100000, 200000, float('inf')],\n    labels=[0, 1, 2, 3, 4],\n    include_lowest=True\n).astype(int)\n\n    \n    # 3. Credit score categories\n    processed_df['CreditScoreCategory'] = pd.cut(\n    processed_df['CreditScore'],\n    bins=[0, 600, 700, 800, 900],\n    labels=[0, 1, 2, 3],\n    include_lowest=True\n).astype(int)\n\n    \n    # 4. Salary categories\n    processed_df['SalaryCategory'] = pd.cut(\n    processed_df['EstimatedSalary'],\n    bins=[0, 50000, 100000, 150000, float('inf')],\n    labels=[0, 1, 2, 3],\n    include_lowest=True\n).astype(int)\n\n\n    \n    # 5. Balance per product ratio\n    processed_df['BalancePerProduct'] = processed_df['Balance'] / (processed_df['NumOfProducts'] + 0.001)\n    \n    # 6. Tenure to age ratio\n    processed_df['TenureAgeRatio'] = processed_df['Tenure'] / processed_df['Age']\n    \n    # 7. Is senior citizen (Age > 60)\n    processed_df['IsSenior'] = (processed_df['Age'] > 60).astype(int)\n    \n    # 8. Has high balance (Balance > 100k)\n    processed_df['HasHighBalance'] = (processed_df['Balance'] > 100000).astype(int)\n    \n    # 9. Credit score to age ratio\n    processed_df['CreditAgeRatio'] = processed_df['CreditScore'] / processed_df['Age']\n    \n    # 10. Product engagement score\n    processed_df['EngagementScore'] = (processed_df['HasCrCard'] + \n                                      processed_df['IsActiveMember'] + \n                                      processed_df['NumOfProducts']) / 3\n    \n    # 11. Financial stability score\n    processed_df['FinancialStability'] = (processed_df['Balance'] / 100000 + \n                                         processed_df['EstimatedSalary'] / 100000) / 2\n    \n    # 12. Tenure category\n    processed_df['TenureCategory'] = pd.cut(\n    processed_df['Tenure'],\n    bins=[0, 2, 5, 8, 10],\n    labels=[0, 1, 2, 3],\n    include_lowest=True\n).astype(int)\n\n    \n    # 13. High value customer indicator\n    processed_df['IsHighValue'] = ((processed_df['Balance'] > 100000) | \n                                  (processed_df['EstimatedSalary'] > 150000)).astype(int)\n    \n    # Label encode categorical variables\n    le_geography = LabelEncoder()\n    le_gender = LabelEncoder()\n    \n    processed_df['Geography'] = le_geography.fit_transform(processed_df['Geography'])\n    processed_df['Gender'] = le_gender.fit_transform(processed_df['Gender'])\n    \n    return processed_df, le_geography, le_gender\n\n# Preprocess data\ndf_processed, le_geo, le_gen = preprocess_data(data)\n\nprint(f\"\\nAfter feature engineering: {df_processed.shape}\")\nprint(f\"Total features created: {df_processed.shape[1] - 1}\")  # -1 for target variable\n\n# Prepare features and target\nX = df_processed.drop('Exited', axis=1)\ny = df_processed['Exited']\n\nprint(f\"\\nFeature columns ({len(X.columns)}):\")\nprint(X.columns.tolist())\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"\\nDataset split:\")\nprint(f\"Training: {X_train.shape[0]} samples ({y_train.mean()*100:.2f}% churn)\")\nprint(f\"Testing: {X_test.shape[0]} samples ({y_test.mean()*100:.2f}% churn)\")\n\n# Feature scaling for algorithms that need it\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Model evaluation function\ndef evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n    \"\"\"Comprehensive model evaluation\"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    auc = roc_auc_score(y_true, y_pred_proba)\n    \n    print(f\"Accuracy: {accuracy*100:.2f}%\")\n    print(f\"Precision: {precision*100:.2f}%\")\n    print(f\"Recall: {recall*100:.2f}%\")\n    print(f\"F1-Score: {f1*100:.2f}%\")\n    print(f\"ROC-AUC: {auc:.4f}\")\n    \n    return accuracy, precision, recall, f1, auc\n\n# Store results\nresults = {}\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"IMPLEMENTING 8 MACHINE LEARNING ALGORITHMS\")\nprint(\"=\"*80)\n\n# 1. LOGISTIC REGRESSION\nprint(\"\\n\" + \"=\"*50)\nprint(\"1. LOGISTIC REGRESSION (Baseline)\")\nprint(\"=\"*50)\n\nlr_model = LogisticRegression(random_state=42, max_iter=1000)\nlr_model.fit(X_train_scaled, y_train)\nlr_pred = lr_model.predict(X_test_scaled)\nlr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n\nlr_metrics = evaluate_model(y_test, lr_pred, lr_pred_proba, 'Logistic Regression')\nresults['Logistic Regression'] = lr_metrics\n\n# 2. RANDOM FOREST\nprint(\"\\n\" + \"=\"*50)\nprint(\"2. RANDOM FOREST\")\nprint(\"=\"*50)\n\nrf_model = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=15,\n    min_samples_split=5,\n    min_samples_leaf=2,\n    random_state=42,\n    n_jobs=-1\n)\n\nrf_model.fit(X_train, y_train)\nrf_pred = rf_model.predict(X_test)\nrf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\nrf_metrics = evaluate_model(y_test, rf_pred, rf_pred_proba, 'Random Forest')\nresults['Random Forest'] = rf_metrics\n\n# 3. XGBOOST\nprint(\"\\n\" + \"=\"*50)\nprint(\"3. XGBOOST\")\nprint(\"=\"*50)\n\nxgb_model = XGBClassifier(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    n_jobs=-1,\n    eval_metric='logloss'\n)\n\nxgb_model.fit(X_train, y_train)\nxgb_pred = xgb_model.predict(X_test)\nxgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n\nxgb_metrics = evaluate_model(y_test, xgb_pred, xgb_pred_proba, 'XGBoost')\nresults['XGBoost'] = xgb_metrics\n\n# 4. LIGHTGBM\nprint(\"\\n\" + \"=\"*50)\nprint(\"4. LIGHTGBM\")\nprint(\"=\"*50)\n\nlgb_model = LGBMClassifier(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    n_jobs=-1,\n    verbose=-1\n)\n\nlgb_model.fit(X_train, y_train)\nlgb_pred = lgb_model.predict(X_test)\nlgb_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n\nlgb_metrics = evaluate_model(y_test, lgb_pred, lgb_pred_proba, 'LightGBM')\nresults['LightGBM'] = lgb_metrics\n\n# 5. CATBOOST\nprint(\"\\n\" + \"=\"*50)\nprint(\"5. CATBOOST\")\nprint(\"=\"*50)\n\ncatboost_model = CatBoostClassifier(\n    iterations=100,\n    learning_rate=0.1,\n    depth=6,\n    random_seed=42,\n    verbose=False\n)\n\ncatboost_model.fit(X_train, y_train)\ncatboost_pred = catboost_model.predict(X_test)\ncatboost_pred_proba = catboost_model.predict_proba(X_test)[:, 1]\n\ncatboost_metrics = evaluate_model(y_test, catboost_pred, catboost_pred_proba, 'CatBoost')\nresults['CatBoost'] = catboost_metrics\n\n# 6. SUPPORT VECTOR MACHINE\nprint(\"\\n\" + \"=\"*50)\nprint(\"6. SUPPORT VECTOR MACHINE (SVM)\")\nprint(\"=\"*50)\n\nsvm_model = SVC(\n    kernel='rbf',\n    C=1.0,\n    gamma='scale',\n    probability=True,\n    random_state=42\n)\n\nsvm_model.fit(X_train_scaled, y_train)\nsvm_pred = svm_model.predict(X_test_scaled)\nsvm_pred_proba = svm_model.predict_proba(X_test_scaled)[:, 1]\n\nsvm_metrics = evaluate_model(y_test, svm_pred, svm_pred_proba, 'SVM')\nresults['SVM'] = svm_metrics\n\n# 7. NEURAL NETWORK\nprint(\"\\n\" + \"=\"*50)\nprint(\"7. NEURAL NETWORK (MLPClassifier)\")\nprint(\"=\"*50)\n\nnn_model = MLPClassifier(\n    hidden_layer_sizes=(100, 50),\n    activation='relu',\n    solver='adam',\n    alpha=0.01,\n    max_iter=1000,\n    random_state=42\n)\n\nnn_model.fit(X_train_scaled, y_train)\nnn_pred = nn_model.predict(X_test_scaled)\nnn_pred_proba = nn_model.predict_proba(X_test_scaled)[:, 1]\n\nnn_metrics = evaluate_model(y_test, nn_pred, nn_pred_proba, 'Neural Network')\nresults['Neural Network'] = nn_metrics\n\n# 8. ENSEMBLE MODEL (VOTING CLASSIFIER)\nprint(\"\\n\" + \"=\"*50)\nprint(\"8. ENSEMBLE MODEL (Voting Classifier)\")\nprint(\"=\"*50)\n\n# Create pipelines for models that need scaling\nlr_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lr', LogisticRegression(random_state=42, max_iter=1000))\n])\n\nsvm_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm', SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42))\n])\n\nnn_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('nn', MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', \n                        solver='adam', alpha=0.01, max_iter=1000, random_state=42))\n])\n\n# Create ensemble with 6 best performing models\nensemble_model = VotingClassifier([\n    ('rf', rf_model),\n    ('xgb', xgb_model),\n    ('lgb', lgb_model),\n    ('catboost', catboost_model),\n    ('svm', svm_pipeline),\n    ('nn', nn_pipeline)\n], voting='soft')\n\nensemble_model.fit(X_train, y_train)\nensemble_pred = ensemble_model.predict(X_test)\nensemble_pred_proba = ensemble_model.predict_proba(X_test)[:, 1]\n\nensemble_metrics = evaluate_model(y_test, ensemble_pred, ensemble_pred_proba, 'Ensemble')\nresults['Ensemble'] = ensemble_metrics\n\n# RESULTS COMPARISON\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPREHENSIVE RESULTS COMPARISON\")\nprint(\"=\"*80)\n\n# Create results dataframe\nresults_df = pd.DataFrame.from_dict(results, orient='index', \n                                   columns=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'])\n\n# Sort by F1-Score\nresults_df = results_df.sort_values('F1-Score', ascending=False)\n\nprint(results_df.round(4))\n\nprint(f\"\\nüèÜ BEST MODEL (F1-Score): {results_df.index[0]}\")\nbest_model = results_df.iloc[0]\nprint(f\"   üìä Accuracy: {best_model['Accuracy']*100:.2f}%\")\nprint(f\"   üéØ Precision: {best_model['Precision']*100:.2f}%\")\nprint(f\"   üîç Recall: {best_model['Recall']*100:.2f}%\")\nprint(f\"   ‚öñÔ∏è  F1-Score: {best_model['F1-Score']*100:.2f}%\")\nprint(f\"   üìà ROC-AUC: {best_model['ROC-AUC']:.4f}\")\n\n# Calculate False Positive Rate for best models\nprint(f\"\\nüìâ FALSE POSITIVE RATE ANALYSIS:\")\nfor model_name in results_df.index[:3]:  # Top 3 models\n    precision = results_df.loc[model_name, 'Precision']\n    # FPR approximation: (1 - precision) * positive_predictions / total_negative\n    fpr_approx = (1 - precision) * 100\n    print(f\"   {model_name}: ~{fpr_approx:.1f}%\")\n\n# NOTE:\n# Ensemble cross-validation is computationally expensive on Kaggle CPU.\n# Commented out for faster execution.\n# Single train-test evaluation above is sufficient for model comparison.\n\n# CROSS-VALIDATION FOR ENSEMBLE\n#print(\"\\n\" + \"=\"*50)\n#print(\"5-FOLD CROSS-VALIDATION (ENSEMBLE)\")\n#print(\"=\"*50)\n\n#cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Cross-validation scores\n#cv_accuracy = cross_val_score(ensemble_model, X, y, cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n#cv_precision = cross_val_score(ensemble_model, X, y, cv=cv_strategy, scoring='precision', n_jobs=-1)\n#cv_recall = cross_val_score(ensemble_model, X, y, cv=cv_strategy, scoring='recall', n_jobs=-1)\n#cv_f1 = cross_val_score(ensemble_model, X, y, cv=cv_strategy, scoring='f1', n_jobs=-1)\n#cv_auc = cross_val_score(ensemble_model, X, y, cv=cv_strategy, scoring='roc_auc', n_jobs=-1)\n\n#print(f\"Cross-Validation Results:\")\n#print(f\"Accuracy:  {cv_accuracy.mean():.4f} (+/- {cv_accuracy.std() * 2:.4f})\")\n#print(f\"Precision: {cv_precision.mean():.4f} (+/- {cv_precision.std() * 2:.4f})\")\n#print(f\"Recall:    {cv_recall.mean():.4f} (+/- {cv_recall.std() * 2:.4f})\")\n#print(f\"F1-Score:  {cv_f1.mean():.4f} (+/- {cv_f1.std() * 2:.4f})\")\n#print(f\"ROC-AUC:   {cv_auc.mean():.4f} (+/- {cv_auc.std() * 2:.4f})\")\n\n# FEATURE IMPORTANCE (from Random Forest)\nprint(\"\\n\" + \"=\"*50)\nprint(\"TOP 10 MOST IMPORTANT FEATURES\")\nprint(\"=\"*50)\n\nfeature_importance = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': rf_model.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(feature_importance.head(10).to_string(index=False))\n\n# FINAL MODEL PERFORMANCE SUMMARY\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéØ FINAL MODEL PERFORMANCE SUMMARY\")\nprint(\"=\"*80)\n\nprint(f\"‚úÖ Dataset Size: {len(data):,} customers\")\nprint(f\"‚úÖ Features Engineered: 13 custom features\")\nprint(f\"‚úÖ Algorithms Implemented: 8 (LR, RF, XGB, LGB, CatBoost, SVM, NN, Ensemble)\")\nprint(f\"‚úÖ Best Model: {results_df.index[0]}\")\nprint(f\"‚úÖ Best F1-Score: {results_df.iloc[0]['F1-Score']*100:.1f}%\")\nprint(f\"‚úÖ Cross-Validation: 5-fold stratified\")\nprint(f\"‚úÖ Model Improvement: {((results_df.iloc[0]['F1-Score'] - results['Logistic Regression'][3])/results['Logistic Regression'][3]*100):.1f}% over baseline\")\n\nprint(f\"\\nüöÄ PRODUCTION READY: {results_df.index[0]} Model\")\nprint(f\"   Ready for deployment with {results_df.iloc[0]['F1-Score']*100:.1f}% F1-Score!\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL TRAINING COMPLETED SUCCESSFULLY! üéâ\")\nprint(\"=\"*80)","metadata":{"papermill":{"duration":10.81258,"end_time":"2025-09-25T17:28:11.129957","exception":true,"start_time":"2025-09-25T17:28:00.317377","status":"failed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"a742fea3","cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"dd467bd0","cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e875bd31","cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"0bb1fc8b","cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}